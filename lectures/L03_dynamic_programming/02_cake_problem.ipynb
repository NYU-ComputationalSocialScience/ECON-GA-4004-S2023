{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Cake Eating Problem\n",
    "\n",
    "Some of this content came from the great [QuantEcon lectures on the](https://python.quantecon.org/cake_eating_problem.html) [cake eating problem](https://python.quantecon.org/cake_eating_numerical.html)!\n",
    "\n",
    "The intertemporal problem presented by the cake eating problem is: how much to enjoy today and how much to leave\n",
    "for the future?\n",
    "\n",
    "Although the topic sounds trivial, this kind of trade-off between current and future utility is at the heart of many economic problems. Once we master the ideas in this simple environment, we will apply them to progressively more challenging—and useful—problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.interpolate as interp\n",
    "import scipy.optimize as opt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Problem definition\n",
    "\n",
    "We consider an infinite time horizon $t = 0, 1, 2, 3\\dots$\n",
    "\n",
    "At $t = 0$ the agent is given a cake with size $\\bar{x}$.\n",
    "\n",
    "The agent must choose how much cake to consume at each time period, $c_t$, with the restriction that\n",
    "\n",
    "$$\\sum_{t=0}^\\infty c_t \\leq \\bar{x}$$\n",
    "\n",
    "Consuming quantity $c$ of the cake gives current utility $u(c)$.\n",
    "\n",
    "The agent discounts the future with discount factor $\\beta$ so that their total utility over the infinite time horizon is\n",
    "\n",
    "$$v(\\bar{x}) = \\sum_{t=0}^{\\infty} \\beta^t u(c_t)$$\n",
    "\n",
    "We stick with the commonly used CRRA utility function\n",
    "\n",
    "$$u(c) = \\frac{c^{1-\\gamma}}{1-\\gamma} \\qquad (\\gamma > 1)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Understanding the parameters\n",
    "\n",
    "Let's take a brief pause to think about the implications that different parameters will have:\n",
    "\n",
    "* $\\beta$: As we increase $\\beta$, the individual will weights the future flow utility more. This means that they should consume less today and more tomorrow -- In fact, as $\\beta \\rightarrow 1$, the individual will want to consume an equal amount each period.\n",
    "* $\\gamma$: As we increase $\\gamma$, we add curvature to the utility function which raises the marginal utility. A higher marginal utility means that utility falls faster with consumption which should increase the amount of consumption smoothing across periods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Directly solving the sequential problem\n",
    "\n",
    "In such a simple setting (and with CRRA utility!), we can actually directly solve the sequential problem...\n",
    "\n",
    "This is useful in the sense that it will give us a benchmark to check the solution that we obtain via dynamic programming. We can set up the Lagrangian to get\n",
    "\n",
    "\\begin{align*}\n",
    "  \\mathcal{L} &= \\sum_{t=0}^{\\infty} \\left[ \\beta^t u(c_t) \\right] - \\lambda \\left(\\sum_{t=0}^{\\infty} [c_t] - \\bar{x} \\right)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Taking the derviative w.r.t. $c_t$ provides\n",
    "\n",
    "\\begin{align*}\n",
    "  \\lambda &= \\beta^t u'(c_t)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We can combine this with the derivative w.r.t. $c_{t+1}$ to get an equation that we will call the Euler equation\n",
    "\n",
    "\\begin{align*}\n",
    "  \\beta &= \\frac{u'(c_{t})}{u'(c_{t+1})}\n",
    "\\end{align*}\n",
    "\n",
    "which after we plug in functional forms can be reduced to\n",
    "\n",
    "\\begin{align*}\n",
    "  \\frac{c_{t+1}}{c_t} &= \\beta^{\\frac{1}{\\gamma}}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This gives us a law of motion for $c_t$,\n",
    "\n",
    "\\begin{align*}\n",
    "  c_{t+1} &= \\beta^{\\frac{1}{\\gamma}} c_{t}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Monotonicity in the utility function implies that the agent will \"want to eat everything eventually\", i.e. $\\sum_{t=0}^\\infty c_t = \\bar{x}$.\n",
    "\n",
    "This can then be used to back out an expression for $c_0$ which can then be used to compute $c_t$\n",
    "\n",
    "\\begin{align*}\n",
    "  \\sum_{t=0}^{\\infty} c_t &= \\bar{x} \\\\\n",
    "  \\sum_{t=0}^{\\infty} \\beta^{\\frac{t}{\\gamma}} c_0 &= \\bar{x} \\\\\n",
    "  \\frac{1}{1 - \\beta^{\\frac{1}{\\gamma}}} c_0 &= \\bar{x} \\\\\n",
    "  c_0 &= (1 - \\beta^{\\frac{1}{\\gamma}}) \\bar{x} \\\\\n",
    "  c_t &= \\beta^{\\frac{t}{\\gamma}} (1 - \\beta^{\\frac{1}{\\gamma}}) \\bar{x}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We can verify this solution numerically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "beta = 0.75\n",
    "gamma = 2\n",
    "xbar = 1.0\n",
    "\n",
    "c0 = (1 - beta**(1/gamma)) * xbar\n",
    "ct_sequence = [beta**(t/gamma)*c0 for t in range(10_000_000)]\n",
    "\n",
    "sum(ct_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Setting up the dynamic program\n",
    "\n",
    "Let's begin by looking at the total utility that an individual receives,\n",
    "\n",
    "$$v(\\bar{x}) = \\sum_{t=0}^{\\infty} \\beta^t u(c_t)$$\n",
    "\n",
    "This can be separated into today's utility (flow value) and all utility going forward (continuation value)\n",
    "\n",
    "$$v(\\bar{x}) = \\underbrace{u(c_0)}_{\\text{flow utility}} + \\underbrace{\\beta \\sum_{t=1}^{\\infty} \\beta^{t-1} u(c_t)}_{\\text{continuation utility}}$$\n",
    "\n",
    "The continuation value looks very similar to the original expression for $v(\\bar{x})$ except that once we've consumed $c_0$, we will only have $\\bar{x} - c_0$ going forward.\n",
    "\n",
    "We make the observation that the value of all future utility can be summarized by just how much cake is left $x_{t+1} = x_t - c_t$.\n",
    "\n",
    "We use this fact to write the Bellman equation as:\n",
    "\n",
    "\\begin{align*}\n",
    "  v(x_t) &= \\max_{0 \\leq c_t  \\leq x} u(c_t) + \\beta v(x_t - c_t)\n",
    "\\end{align*}\n",
    "\n",
    "A solution to the dynamic program consists of two functions:\n",
    "\n",
    "1. A value function, $v^*(x): \\mathcal{R}^+ \\rightarrow \\mathcal{R}$. The value function describes the value to the agent of beginning a particular period with $x$ cake remaining\n",
    "2. A policy function, $c^*(x): \\mathcal{R}^+ \\rightarrow [0, x]$. The policy function describes the optimal level of consumption that an individual should choose given $x$ cake remaining\n",
    "\n",
    "These two functions must satisfy the Bellman equation and $c^*(x)$ must be the maximizing choice of consumption."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Solving the dynamic program\n",
    "\n",
    "We will discuss two strategies for solving this dynamic program. Both rely on certain conditions being met, but we will postpone this technical discussion for now (we promise the conditions are met for the cake eating problem we describe):\n",
    "\n",
    "1. Value iteration: Treats the value function, $v(x)$, as the object of interest\n",
    "2. Policy iteration: Treats the policy function, $c(x)$, as the object of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Define class for convenience of methods and parameter passing\n",
    "class CakeEating(object):\n",
    "    def __init__(self, beta=0.9, gamma=2.0, xbar=2.5, nx=75):\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "        self.xbar = xbar\n",
    "        self.xgrid = np.linspace(1e-2, xbar, nx)\n",
    "\n",
    "    def u(self, c):\n",
    "        return c**(1 - self.gamma) / (1 - self.gamma)\n",
    "\n",
    "    def action_state_value(self, c, x, v):\n",
    "        \"\"\"\n",
    "        Given a value function, `v`, computes\n",
    "        the value of taking action `c` for\n",
    "        state `x`\n",
    "        \"\"\"\n",
    "        return self.u(c) + self.beta*v(x - c)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Approximating $c$ and $v$\n",
    "\n",
    "The size of the cake is a continuous variable... We won't be able to iterate over all values between 0 and $x$ so instead we choose a grid of $N$ values $x_0 < x_1 < \\dots < x_N$ to use to approximate the function.\n",
    "\n",
    "If we have the value of our function at each of these grid points, then we can numerically approximate the grid by using interpolation. In particular, we will use piecewise linear functions which approximate a function according to the following rules:\n",
    "\n",
    "Let $f$ be a function that we are approximating and $x$ be the value we want to approximate the function at. Let $x_i \\leq x \\leq x_{i+1}$ then\n",
    "\n",
    "$$\\tilde{f}(x) = f(x_i) + \\frac{f(x_{i+1}) - f(x_i)}{x_{i+1} - x_{i}} (x - x_i)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Value Iteration\n",
    "\n",
    "Value iteration begins by treating the value function, $v(x)$, as the object of interest.\n",
    "\n",
    "1. Initialize a guess at an initial state-value function -- It is common to use $v_0(x) = 0 \\;\\forall x$ as an initial guess.\n",
    "2. Compute $v_{n+1}(x) = \\max_{0 \\leq c \\leq x} u(c) + \\beta v_n(x - c)$ for each value $x$\n",
    "3. Compare whether new value is approximately equal to previous value -- If so, finish, otherwise return to 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def bellman_equation(ce, x, v):\n",
    "    \"\"\"\n",
    "    Given a value function `v`, computes the maximum value\n",
    "    associated with being in state x.\n",
    "    \n",
    "    For convenience, it also returns the optimal decision made\n",
    "    \"\"\"\n",
    "    # Optimize \n",
    "    sol = opt.minimize_scalar(\n",
    "        lambda c: -ce.action_state_value(c, x, v),\n",
    "        bounds=(1e-8, x-1e-10), method=\"bounded\"\n",
    "    )\n",
    "\n",
    "    return sol.x, -sol.fun\n",
    "\n",
    "\n",
    "def value_iteration(ce):\n",
    "    \"\"\"\n",
    "    Computes the optimal policy function and value function\n",
    "    via value iteration\n",
    "    \"\"\"\n",
    "    v_np1 = np.zeros_like(ce.xgrid)\n",
    "    v_n = v_np1.copy() + 100.0\n",
    "    c_star = np.zeros_like(ce.xgrid)\n",
    "\n",
    "    dist = 100.0\n",
    "    citer = 0\n",
    "    while dist > 1e-3:\n",
    "        citer += 1\n",
    "        if citer % 25 == 0:\n",
    "            print(f\"Current iteration: {citer}\")\n",
    "            print(f\"Current distance: {dist}\")\n",
    "\n",
    "        # Create interpolator\n",
    "        np.copyto(v_n, v_np1)\n",
    "        v = interp.interp1d(\n",
    "            ce.xgrid, v_n,\n",
    "            fill_value=\"extrapolate\"\n",
    "        )\n",
    "\n",
    "        for (i, x) in enumerate(ce.xgrid):\n",
    "            _c, _v = bellman_equation(ce, x, v)\n",
    "            c_star[i] = _c\n",
    "            v_np1[i] = _v\n",
    "        dist = np.max(np.abs(v_np1 - v_n))\n",
    "    \n",
    "    return c_star, v_np1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "ce = CakeEating()\n",
    "\n",
    "cstar_vf, vstar_vf = value_iteration(ce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2)\n",
    "\n",
    "ax[0].plot(ce.xgrid, vstar_vf)\n",
    "ax[0].set_title(\"Value function\")\n",
    "ax[1].plot(ce.xgrid, cstar_vf)\n",
    "ax[1].set_title(\"Optimal consumption\")\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Policy Iteration\n",
    "\n",
    "Policy iteration begins by treating the policy function, $c(x)$, as the object of interest.\n",
    "\n",
    "1. Initialize a guess for the optimal policy -- We will choose $c_0(x) = \\frac{x}{2}$ which will always respect the budget constraint even if it is clearly suboptimal.\n",
    "2. Take the policy as given and compute $\\tilde{v}_{n}(x)$ as the value an agent would have in state $x$ if the agent followed the policy $c_{n}(x)$.\n",
    "  - This is done by iterating on $v_{n+1}(x) = u(c_0(x)) + \\beta v_{n}(x - c_0(x))$ until $v_n(x) \\approx v_{n+1}(x)$ and then $\\tilde{v}_n(x) \\equiv v_{n+1}(x)$\n",
    "3. Compute the optimal policy for $\\tilde{v}_{n}(x)$ and call this $c_{n+1}(x)$\n",
    "3. Compare whether the new policy is approximately equal to previous policy -- If so, finish, otherwise return to 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def value_given_policy(ce, x, c, v):\n",
    "    \"\"\"\n",
    "    Given a consumption policy function and\n",
    "    a value function, what's the value from\n",
    "    using that policy\n",
    "    \"\"\"\n",
    "    _c = c(x)\n",
    "    \n",
    "    return ce.action_state_value(_c, x, v)\n",
    "\n",
    "\n",
    "def iterate_given_policy(ce, c):\n",
    "    \"\"\"\n",
    "    Given a particular policy, computes the value of\n",
    "    following that policy indefinitely\n",
    "    \"\"\"\n",
    "    v_np1 = np.zeros_like(ce.xgrid)\n",
    "    v_n = v_np1.copy()\n",
    "    \n",
    "    citer = 0\n",
    "    dist = 100.0\n",
    "    while dist > 1e-3:\n",
    "        # Create interpolator\n",
    "        np.copyto(v_n, v_np1)\n",
    "        v = interp.interp1d(\n",
    "            ce.xgrid, v_n,\n",
    "            fill_value=\"extrapolate\"\n",
    "        )\n",
    "\n",
    "        for (i, x) in enumerate(ce.xgrid):\n",
    "            _v = value_given_policy(ce, x, c, v)\n",
    "            v_np1[i] = _v\n",
    "\n",
    "        dist = np.max(np.abs(v_np1 - v_n))\n",
    "\n",
    "    return v_np1\n",
    "\n",
    "\n",
    "def update_policy(ce, v, cout):\n",
    "    \"\"\"\n",
    "    Given a particular value function, v, compute\n",
    "    the optimal consumption level\n",
    "    \"\"\"\n",
    "    for (i, x) in enumerate(ce.xgrid):\n",
    "        cpol, _ = bellman_equation(ce, ce.xgrid[i], v)\n",
    "        cout[i] = cpol\n",
    "\n",
    "    return cout\n",
    "\n",
    "\n",
    "def policy_iteration(ce):\n",
    "    \"\"\"\n",
    "    Computes the optimal policy function and value function\n",
    "    via policy iteration\n",
    "    \"\"\"\n",
    "    c_n = ce.xgrid / 2\n",
    "    c_np1 = c_n.copy()\n",
    "\n",
    "    dist = 100.0\n",
    "    citer = 0\n",
    "    while dist > 1e-3:\n",
    "        citer += 1\n",
    "        if citer % 25 == 0:\n",
    "            print(f\"Current iteration: {citer}\")\n",
    "            print(f\"Current distance: {dist}\")\n",
    "\n",
    "        # Create interpolator\n",
    "        np.copyto(c_n, c_np1)\n",
    "        c = interp.interp1d(\n",
    "            ce.xgrid, c_n,\n",
    "            fill_value=\"extrapolate\"\n",
    "        )\n",
    "\n",
    "        # Compute corresponding value of policy c\n",
    "        vtilde_n = iterate_g iven_policy(ce, c)\n",
    "        vtilde = interp.interp1d(\n",
    "            ce.xgrid, vtilde_n,\n",
    "            fill_value=\"extrapolate\"\n",
    "        )\n",
    "\n",
    "        c_np1 = update_policy(ce, vtilde, c_np1)\n",
    "        dist = np.max(np.abs(c_np1 - c_n))\n",
    "    \n",
    "    return c_np1, vtilde_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "cstar_pf, vstar_pf = policy_iteration(ce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2)\n",
    "\n",
    "ax[0].plot(ce.xgrid, vstar_pf)\n",
    "ax[0].set_title(\"Value function\")\n",
    "ax[1].plot(ce.xgrid, cstar_pf)\n",
    "ax[1].set_title(\"Optimal consumption\")\n",
    "\n",
    "fig.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
