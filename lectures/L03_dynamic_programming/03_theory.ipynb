{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Dynamic Programming: Theory and Tools\n",
    "\n",
    "This lecture today references material from:\n",
    "\n",
    "* [QuantEcon lectures](https://quantecon.org/lectures/)\n",
    "* [Dynamic Economics by Jerome Adda and Russell Cooper](https://mitpress.mit.edu/books/dynamic-economics)\n",
    "* [Economic Dynamics: Theory and Computation by John Stachurski](https://mitpress.mit.edu/books/economic-dynamics)\n",
    "\n",
    "We've previously referenced the first, but the two books mentioned here are also of exceptional quality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.interpolate as interp\n",
    "import scipy.stats as st\n",
    "import quantecon as qe\n",
    "\n",
    "from ipywidgets import interact, IntSlider\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What have we learned so far?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Two applications\n",
    "\n",
    "1. Shortest path: How do we find the shortest path connecting two pre-specified nodes on a (directed) graph?\n",
    "2. Cake eating: How should an indidivual divide consumption of a cake over an infinite horizon?\n",
    "\n",
    "The solutions to these two problems consisted of two functions:\n",
    "\n",
    "1. Policy function: This is a rule that specifies what action to take given the \"state\"\n",
    "  - In the shortest path problem, the policy function specifies which edge to follow (to move to the next node) given the node that you're currently at.\n",
    "  - In the cake eating problem, the policy function specifies how much of the cake I should eat today given how much cake I have to begin the day.\n",
    "2. Value function: This specifies the value (or cost) of following a particular policy function\n",
    "  - In the shortest path problem, the value function tells us how much it will \"cost\" to follow the policy that gets us to the terminal node.\n",
    "  - In the cake eating problem, this specifies the total utility, $\\sum_t u(c_t)$, of having a particular amount of cake and following the given policy function going forward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Two algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Value function iteration**\n",
    "\n",
    "Value function iteration focuses on finding the optimal value function.\n",
    "\n",
    "We described (and used!) value function iteration in the context of both problems.\n",
    "\n",
    "The algorithm is structured as,\n",
    "\n",
    "1. Make a guess at the value function\n",
    "2. Update the value function according to the Bellman equation\n",
    "3. Check for convergence:\n",
    "  - If the updated value function is \"close enough\" to the current value function, use that as the optimal value function\n",
    "  - Otherwise, use the updated value function as the input to step 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Policy function iteration**\n",
    "\n",
    "Policy function iteration focuses on finding the optimal policy function.\n",
    "\n",
    "We described (and used!) policy function iteration in the context of the cake eating problem.\n",
    "\n",
    "The algorithm is structured as,\n",
    "\n",
    "1. Make a guess at the policy function\n",
    "2. Find the value function associated with the specified policy function (by iterating on the value function).\n",
    "3. Find the optimal policy function for the value function from step 2.\n",
    "3. Check for convergence:\n",
    "  - If the updated policy function is \"close enough\" to the current policy function, use that as the optimal policy function (and the value function from step 2 as the optimal value function)\n",
    "  - Otherwise, use the updated policy function as the input to step 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### New tools\n",
    "\n",
    "**Interpolation**\n",
    "\n",
    "Value function iteration and policy function iteration both occur in function space because we are iterating on functions.\n",
    "\n",
    "Function space is a complicated thing to represent in a computer so we instead approximate the functions using interpolation on a grid of points. When we evaluate the convergence, rather than compare $||V_j - V_{j+1}||_p$ we compare $||\\begin{bmatrix} V_j(x_0) & V_j(x_1) & \\dots & V_j(x_n) \\end{bmatrix} - \\begin{bmatrix} V_{j+1}(x_0) & V_{j+1}(x_1) & \\dots & V_{j+1}(x_n) \\end{bmatrix}||_p$\n",
    "\n",
    "Interpolation solves the following problem:\n",
    "\n",
    "> Given a function $f$, $n$ points $x \\in X$, and function evaluations at those points, $f(x) \\; \\forall x \\in X$, construct an approximate function $\\tilde{f}$ from a pre-specified class of functions that can be used to evaluate the function at $x \\notin X$.\n",
    ">\n",
    "> Interpolation will typically enforce $\\tilde{f}(x) = f(x) \\; \\forall x \\in X$.\n",
    "\n",
    "We briefly discussed piece-wise linear interpolation. We see an example of piece-wise linear interpolation below, but defer a more complete exploration of these tools because entire courses could be taught on interpolation methods and error bounding the function approximations etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def interp_example(n):\n",
    "    # Original data\n",
    "    x = np.linspace(0, 2*np.pi, n)\n",
    "    y = np.sin(x)\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    # Plot the original data\n",
    "    ax.scatter(x, y, color=\"k\")\n",
    "\n",
    "    # Create and plot interpolator\n",
    "    x_interp = np.linspace(0, 2*np.pi, 10*n)\n",
    "    pwl = interp.interp1d(x, y)\n",
    "    y_interp = pwl(x_interp)\n",
    "    ax.plot(x_interp, y_interp, color=\"b\")\n",
    "\n",
    "    # Plot exact function\n",
    "    y_exact = np.sin(x_interp)\n",
    "    ax.plot(x_interp, y_exact, color=\"r\", linewidth=1.0, linestyle=\"--\")\n",
    "\n",
    "    pass\n",
    "\n",
    "interact(\n",
    "    interp_example,\n",
    "    n=IntSlider(min=5, max=25, step=1, value=5)\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Fixed points\n",
    "\n",
    "Value function iteration (and policy function iteration) rely on [fixed point theory](https://en.wikipedia.org/wiki/Fixed_point_(mathematics))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### What is a fixed point?\n",
    "\n",
    "Let $T$ be a function that maps a given space $X$ into itself, i.e. $T: X \\rightarrow X$.\n",
    "\n",
    "A fixed point is an input, $x \\in X$, such that $f(x) = x$ (or, equivalently, $f(x) - x = 0$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Examples**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Example 1: Let $T: \\mathcal{R} \\rightarrow \\mathcal{R}$ be defined by $T(x) = x^2$.\n",
    "\n",
    "Then $T(0) - 0 = 0^2 - 0 = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Example 2: Let $\\mathcal{F}$ represent the space of all functions and define $T: \\mathcal{F} \\rightarrow \\mathcal{F}$ as $T(f) = (x + 0.1) + 0.9 f$. Let $f(x) \\equiv 10x + 1$ then\n",
    "\n",
    "\\begin{align*}\n",
    "  T(f) &= (x + 0.1) + 0.9f \\\\\n",
    "  &= (x + 0.1) + 0.9(10x + 1) \\\\\n",
    "  &= (x + 9x) + (0.1 + 0.9)  \\\\\n",
    "  &= 10x + 1 = f \\\\\n",
    "  &\\rightarrow T(f) = f\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### How do we find fixed points?\n",
    "\n",
    "Well... One choice, as illustrated above, is to guess and check.\n",
    "\n",
    "We might want something slight more robust than that though so we turn to the [contraction mappings](https://en.wikipedia.org/wiki/Contraction_mapping) and [fixed-point theorems](https://en.wikipedia.org/wiki/Fixed-point_theorem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Contraction mapping**\n",
    "\n",
    "A contraction mapping (on a metric space $(M, d)$) is a function $f$ from $M$ to itself with the property that there is some nonnegative real number, $0 \\leq k \\leq 1$ such that for all $x$ and $y$ in $M$, $d(f(x), f(y)) \\leq k d(x, y)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "What does this mean in terms of value function iteration?\n",
    "\n",
    "If we define $T$ as the Bellman operator (used in the update step in value function iteration) and let\n",
    "\n",
    "$$d_0 = d(V_0, T(V_0)) = d(V_0, V_1)$$\n",
    "\n",
    "and\n",
    "\n",
    "$$d_1 = d(T(V_0), T(T(V_0))) = d(V_1, V_2)$$\n",
    "\n",
    "then $d_1 < d_0$. As we continue this sequence, for any $\\varepsilon > 0$, we could find $N$ such that $d(V_N, V_{N+1}) < \\varepsilon$.\n",
    "\n",
    "The above reasoning is what motivates the algorithm driving value function iteration (and policy function iteration)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "The above is effectively a word description of the outcome of the Banach Fixed Point Theorem which states,\n",
    "\n",
    "> Let $(M, d)$ be a non-empty complete metric space with a contraction mapping $T: M \\rightarrow M$. Then $T$ admits a unique fixed-point, $x^* \\in M$. Furthermore, $x^*$ can be found as follows: start with an arbitrary element $x_0 \\in M$ and define a sequence $\\{x_n\\}$ by $x_n = T(x_{n-1})$ for $n \\geq 1$ then $x_n \\rightarrow x^*$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## General formulation of dynamic programming\n",
    "\n",
    "**Notation**\n",
    "\n",
    "- Let $s_t$ be the *state variables*. The state variables tell us everything that we need to know in order to understand the value in period $t$ -- It summarizes the history of all information in the past that we need to make a forward looking decision.\n",
    "- Let $a_t$ be the *control variables*. The control variables are what the agent can directly choose. We can potentially restrict $a_t \\in \\Gamma(s_t)$ where $\\Gamma(s_t)$ is a non-empty compact set. We will often denote a particular policy as $\\sigma(s_t) : S \\rightarrow \\Gamma(S)$.\n",
    "- Let $r(s_t, a_t)$ be the reward function (flow value or flow utility). This notation acknowledges that we are going to focus on additvely separable utility -- i.e. $\\sum_t \\beta^t r(s_t, a_t)$.\n",
    "- Let $F(s_t, a_t)$ be the transition function that expresses how the state variables move in response to the current state and control variables.\n",
    "- Let $\\beta$ be the discount factor with $0 < \\beta < 1$. We could allow $\\beta$ to be equal to 1 in cases where the horizon is finite...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We can then write the general Bellman equation as\n",
    "\n",
    "\\begin{align*}\n",
    "  V(s_t) &= \\max_{a_t \\in \\Gamma(s_t)} r(s_t, a_t) + \\beta V(s_{t+1}) \\\\\n",
    "  &\\text{where }\\\\\n",
    "  &s_{t+1} = F(s_t, a_t) \\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Mapping the general framework to the cake eating problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "* $s_t$ was the size of the cake because that summarized all relevant information needed to make decisions going forward. Finding the state in more complex problems sometimes requires some additional -- \"Finding the state is an art\"\n",
    "* $a_t$ was how much cake to consume today\n",
    "* $r(s_t, a_t)$ was the per-period utility function\n",
    "* $\\Gamma(s_t) = [0, x_t]$ - The agent can't consume more cake than available\n",
    "* $F(s_t, a_t) \\rightarrow x_t - c_t$ was the transition function\n",
    "* $\\beta$ mapped to $\\beta$...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Deterministic vs stochastic\n",
    "\n",
    "What we've described so far only satisfies environments without randomness... What happens when $\\{s_t\\}$ includes a random variable?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Almost exactly the same! The main difference is that we need to update the transition function and take expectations over tomorrow:\n",
    "\n",
    "Rather than use $s_{t+1} = F(s_t, a_t)$, we use $s_{t+1} = F(s_t, a_t, w_{t+1})$ where $w_{t+1}$ is a random variable realized in period $t+1$ and so is not known in $t$.\n",
    "\n",
    "The Bellman equation becomes\n",
    "\n",
    "\\begin{align*}\n",
    "  V(s_t) &= \\max_{a_t \\in \\Gamma(s_t)} r(s_t, a_t) + \\beta E \\left[ V(s_{t+1}) \\right] \\\\\n",
    "  &\\text{where }\\\\\n",
    "  &s_{t+1} = F(s_t, a_t, w_{t+1}) \\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Numerically computing expectations\n",
    "\n",
    "In stochastic dynamic programming, we will need to be able to evaluate expectations of random variables.\n",
    "\n",
    "To keep things \"simple\", we are going to work though the examples of evaluating expectations of $g(x_{t+1}) = \\sqrt{|x_{t+1}|}$ for different random variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Discrete random variables\n",
    "\n",
    "In the case of a discrete random variable, evaluating an expectation reduces to summations and multiplications... Luckily, computers are very good at summing and multiplying.\n",
    "\n",
    "Let\n",
    "\n",
    "\\begin{align*}\n",
    "  x_{t+1} \\sim \\begin{cases}\n",
    "    0.0 \\; \\text{ with probability } 0.25 \\\\\n",
    "    2.5 \\; \\text{ with probability } 0.25 \\\\\n",
    "    5.0 \\; \\text{ with probability } 0.25 \\\\\n",
    "    25.0 \\; \\text{ with probability } 0.25 \\\\\n",
    "  \\end{cases}\n",
    "\\end{align*}\n",
    "\n",
    "then $E[g(x_{t+1})] = \\sum_i p(x_i) g(x_i) = \\sum_i 0.25 \\sqrt{|x_i|}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "probabilities = np.ones(4)*0.25\n",
    "values = np.array([0.0, 2.5, 5.0, 25.0])\n",
    "\n",
    "E_drv = np.sum(probabilities * np.sqrt(np.abs(values)))\n",
    "E_drv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Continuous random variables\n",
    "\n",
    "In the case of continuous random variables, evaluating an expectation becomes integration. Exactly evaluating an integral is not a trivial exercise and there are many integrals that cannot be exactly evaluated even by a human...\n",
    "\n",
    "We will need to resort to an approximation and there are two main ways forward:\n",
    "\n",
    "1. Monte Carlo\n",
    "2. Quadrature\n",
    "\n",
    "In the examples that follow, we will assume that $x_{t+1} \\sim N(0, 1)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Monte Carlo**\n",
    "\n",
    "[Monte Carlo integration](https://en.wikipedia.org/wiki/Monte_Carlo_integration) relies on the law of large numbers. If we draw enough samples from our random variable and evaluate the function on those random variables then in the limit, we should be able to evaluate our expectation.\n",
    "\n",
    "In practice, this works well enough for low dimensional objects, but can become prohibitively expensive as the number of dimensions expands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def monte_carlo_example(n):\n",
    "    d = st.norm()\n",
    "    draws = d.rvs(n)\n",
    "    x = np.linspace(-3.5, 3.5, 500)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    # Plot the original data\n",
    "    ax.hist(draws, density=True)\n",
    "    ax.plot(x, d.pdf(x), color=\"k\", linestyle=\"--\")\n",
    "\n",
    "    print(f\"Approximated expectation is {np.mean(np.sqrt(np.abs(draws)))}\")\n",
    "    pass\n",
    "\n",
    "interact(\n",
    "    monte_carlo_example,\n",
    "    n=IntSlider(min=25, max=10_000, step=250, value=25)\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Quadrature**\n",
    "\n",
    "[Quadrature](https://en.wikipedia.org/wiki/Quadrature_(mathematics)) is a class of methods used to approximate the area under a curve (aka integrating...).\n",
    "\n",
    "Given a function, $h(x)$, quadrature approximates the area under a curve by finding values $\\{x_i\\}_i$ and weights $\\{w_i\\}_i$ such that\n",
    "\n",
    "$$\\int_a^b h(x) dx \\approx \\sum_i w_i h(x_i)$$\n",
    "\n",
    "One of the assumptions typically required for quadrature to work well is that the function can be approximated well by polynomials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def gaussian_quadrature(n):\n",
    "    d = st.norm()\n",
    "    vals, weights = qe.quad.qnwnorm(n, 0.0, 1.0)\n",
    "\n",
    "    y = np.sqrt(np.abs(vals))\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    # Plot the original data\n",
    "    ax.scatter(vals, weights, color=\"k\", linestyle=\"--\")\n",
    "    ax.plot(vals, y*weights, color=\"k\")\n",
    "\n",
    "    print(f\"Approximated expectation is {np.sum(weights*y)}\")\n",
    "    pass\n",
    "\n",
    "interact(\n",
    "    gaussian_quadrature,\n",
    "    n=IntSlider(min=5, max=75, step=5, value=5)\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "In this case, the integral is hard to approximate because it isn't well approximated by a polynomial -- It's non-differentiable."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
